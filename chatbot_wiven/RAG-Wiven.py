import os
import json
import gradio as gr

from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings  # ğŸš€ nouveau package
from langchain_ollama import ChatOllama                  # ğŸš€ nouveau package
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CONFIGURATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

JSON_DIR = "docs"  # ğŸ“ RÃ©pertoire contenant vos fichiers .json

# Liste des fichiers disponibles
json_files = [f for f in os.listdir(JSON_DIR) if f.endswith(".json")]
if not json_files:
    raise FileNotFoundError(f"Aucun fichier .json trouvÃ© dans {JSON_DIR}.")

display_names = [f.removesuffix(".json") for f in json_files]
name2file = dict(zip(display_names, json_files))


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FONCTIONS UTILITAIRES
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_chain(json_file: str) -> RetrievalQA:
    """Construit (ou reconstruit) la chaÃ®ne QA pour le fichier demandÃ©."""

    with open(os.path.join(JSON_DIR, json_file), "r", encoding="utf-8") as f:
        data = json.load(f)

    # Mise en forme dâ€™une vue ERP en texte libre
    def format_vue(d: dict) -> str:
        lines: list[str] = [f"Titre : {d.get('titre', '')}"]
        lines += ["\nApplications/Programmes :", *d.get("applications", [])]
        lines += ["\nFonctionnalitÃ©s :", *d.get("fonctionnalites", [])]
        lines += ["\nBoutons :", *d.get("champs_et_boutons", {}).get("boutons", [])]
        lines += ["Cases Ã  cocher :", *d.get("champs_et_boutons", {}).get("cases_a_cocher", [])]
        lines += ["\nDescription :", d.get("description", "")]
        return "\n".join(lines)

    # 1ï¸âƒ£Â DÃ©coupe le document en chunks
    doc = Document(page_content=format_vue(data))
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = splitter.split_documents([doc])

    # 2ï¸âƒ£Â Vectorise les chunks avec un embedding HuggingFace en CPU
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={"device": "cpu"},
    )
    vectorstore = FAISS.from_documents(chunks, embeddings)

    # 3ï¸âƒ£Â LLM (=Â Mistral) via Ollama + Retriever
    llm = ChatOllama(model="mistral")
    retriever = vectorstore.as_retriever()

    # 4ï¸âƒ£Â ChaÃ®ne QA prÃªte
    return RetrievalQA.from_chain_type(llm=llm, retriever=retriever)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CALLBACKS GRADIO
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def init_chain(selected_name: str):
    """Callback â†’ reconstruit la chaÃ®ne quand la vue change."""
    qa = build_chain(name2file[selected_name])
    return qa, gr.update(value="", interactive=True)


def ask_question(question: str, chat_history: list, qa_chain: RetrievalQA):
    """Envoie la question au modÃ¨le et renvoie la rÃ©ponse."""
    if not question.strip():
        return gr.update(), chat_history

    answer = qa_chain.invoke(question)  # â† .invoke au lieu de .run
    chat_history.append((question, answer))
    return "", chat_history


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# INTERFACE GRADIO (widget intÃ©grable)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

with gr.Blocks(title="Chatbot ERP - Widget", theme=gr.themes.Soft()) as demo:
    gr.Markdown("## ğŸ§  Chatbot ERP\nPosez vos questions sur la vue ERP sÃ©lectionnÃ©e.")

    vue_select = gr.Dropdown(
        choices=display_names,
        value=display_names[0],
        label="ğŸ“‚ Vue ERP",
    )

    # âš ï¸Â lambdaÂ â†’ Ã©vite que Gradio appelle Ã  tort RetrievalQA.__call__
    qa_chain_state = gr.State(lambda: build_chain(name2file[display_names[0]]))

    chatbot = gr.Chatbot(label="Historique")
    question_box = gr.Textbox(
        placeholder="ExÂ : Que fait le bouton ModifierÂ ?",
        label="Votre question",
    )
    submit_btn = gr.Button("Envoyer", variant="primary")

    # ğŸ–‡ï¸Â Ã‰vÃ©nements
    vue_select.change(init_chain, inputs=vue_select, outputs=[qa_chain_state, question_box])
    submit_btn.click(
        ask_question,
        inputs=[question_box, chatbot, qa_chain_state],
        outputs=[question_box, chatbot],
    )
    question_box.submit(
        ask_question,
        inputs=[question_box, chatbot, qa_chain_state],
        outputs=[question_box, chatbot],
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LANCEMENT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    demo.launch()
